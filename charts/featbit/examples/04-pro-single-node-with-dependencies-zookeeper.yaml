### This example includes all dependencies including zookeeper.
### IMPORTANT: if trying this example after example 3, or any deployment of kafka that uses kraft, you will
### likely need to delete the pvc associated with kafka and zookeeper and the ids and version will be out of sync in
### /bitnami/kafka/data/meta.properties as well as in zookeepers data.

isPro: true

apiExternalUrl: "http://127.0.0.1:5000"
evaluationServerExternalUrl: "http://127.0.0.1:5100"
demoExternalUrl: "https://featbit-samples.vercel.app"

# # Settings for Zookeeper.
# # See https://github.com/bitnami/charts/tree/master/bitnami/zookeeper
zookeeper:
  enabled: true
  fullnameOverride: featbit-zookeeper
  # Set 1 for single node, otherwise 3
  replicaCount: 1 
  image:
    registery: docker.io
    repository: bitnami/zookeeper
    tag: 3.8.1-debian-11-r36
    pullPolicy: IfNotPresent
  ## ZooKeeper resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## @param resources.limits The resources limits for the ZooKeeper containers
  ## @param resources.requests.memory The requested memory for the ZooKeeper containers
  ## @param resources.requests.cpu The requested cpu for the ZooKeeper containers
  ##
  resources: {}
#    limits:
#      memory: 2048Gi
#      cpu: 250m
#    requests:
#      memory: 256Mi
#      cpu: 250m
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param podSecurityContext.enabled Enabled ZooKeeper pods' Security Context
  ## @param podSecurityContext.fsGroup Set ZooKeeper pod's Security Context fsGroup
  ##
  podSecurityContext:
    enabled: true
    fsGroup: 1001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## @param containerSecurityContext.enabled Enabled ZooKeeper containers' Security Context
  ## @param containerSecurityContext.runAsUser Set ZooKeeper containers' Security Context runAsUser
  ## @param containerSecurityContext.runAsNonRoot Set ZooKeeper containers' Security Context runAsNonRoot
  ## @param containerSecurityContext.allowPrivilegeEscalation Force the child process to be run as nonprivilege
  ##
  containerSecurityContext:
    enabled: true
    runAsUser: 1001
    runAsNonRoot: true
    allowPrivilegeEscalation: false
  heapsize: 1024
  metrics:
    enabled: false

# Settings for Kafka.
# See https://github.com/bitnami/charts/tree/master/bitnami/kafka
kafka:
  enabled: true
  debug: true
  diagnosticMode:
    enabled: false
  replicaCount: 1
  allowPlaintextListener: true
  defaultReplicationFactor: 1
  offsetsTopicReplicationFactor: 1
  transactionStateLogReplicationFactor: 1
  transactionStateLogMinIsr: 1
  # 50 MB
  maxMessageBytes: "50000000"
  # 50 MB
  socketRequestMaxBytes: "50000000"

  kraft:
    enabled: false

  env:
  - name: KAFKA_BROKER_ID
    value: "1001"
  - name: KAFKA_RESERVED_BROKER_MAX_ID
    value: "100001"
  - name: KAFKA_CFG_RESERVED_BROKER_MAX_ID
    value: "100001"
  - name: KAFKA_NODE_ID
    value: "1001"
  - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
    value: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
  - name: KAFKA_CFG_LISTENERS
    value: PLAINTEXT://:9092,PLAINTEXT_HOST://:29092
  - name: KAFKA_CFG_ADVERTISED_LISTENERS
    value: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
  - name: KAFKA_CFG_ZOOKEEPER_CONNECT
    value: featbit-zookeeper:2181
  - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
    value: 'true'
  - name: ALLOW_PLAINTEXT_LISTENER 
    value: 'true'
  provisioning:
    enabled: "true"
    replicationFactor: 1
    topics:
      - name: "featbit-feature-flag-change"
        # replicationFactor: 1
      - name: "featbit-segment-change"
        # replicationFactor: 1
      - name: "featbit-endusers"
        # replicationFactor: 1
      - name: "featbit-insights"
        # replicationFactor: 1
  zookeeperChrootPath: "/kafka"
  externalZookeeper:
    servers: [featbit-zookeeper]

  ## Use this to enable an extra service account
  # serviceAccount:
  #   create: false
  #   name: kafka

  ## Use this to enable an extra service account
  # zookeeper:
  #   serviceAccount:
  #     create: false
  #     name: zookeeper

## This value is only used when kafka.enabled is set to false
##
externalKafka:
  ## Hostname or ip address of external kafka
  ##
  host: "kafka"
  port: 9092    

# https://github.com/bitnami/charts/blob/main/bitnami/redis/README.md
redis:
  enabled: true
  architecture: standalone
  auth:
    enabled: false
    sentinel: false
  fullnameOverride: featbit-redis
  usePassword: false
  ## Just omit the password field if your redis cluster doesn't use password
  # password: redis
  master:
    persistence:
      enabled: true
  # replica defaults to 2
  replica:
    replicaCount: 1
  ## Use this to enable an extra service account
  # serviceAccount:
  #   create: false
  #   name: featbit-redis

## This value is only used when redis.enabled is set to false
##
externalRedis:
  ## Hostname or ip address of external redis cluster
  ##
  host: "redis"
  port: 6379
  ## Just omit the password field if your redis cluster doesn't use password
  # password: redis


# https://github.com/bitnami/charts/tree/main/bitnami/mongodb
mongodb:
  enabled: true
  architecture: standalone
  useStatefulSet: true
  fullnameOverride: featbit-mongodb
  env:
  - name: MONGO_INITDB_ROOT_USERNAME
    value: admin
  - name: MONGO_INITDB_ROOT_PASSWORD
    value: password
  - name: MONGO_INITDB_DATABASE
    value: featbit
  ports:
  - name: "27017"
    port: 27017
    targetPort: 27017
  type: ClusterIP

## This value is only used when mongodb.enabled is set to false
##
externalmongodb:
  ## Hostname or ip address of external redis cluster
  ##
  host: "mongodb"
  port: 27017
  # username: admin
  ## Just omit the password field if your redis cluster doesn't use password
  # password: mongodb

# https://github.com/bitnami/charts/tree/main/bitnami/clickhouse
# https://github.com/bitnami/charts/blob/main/bitnami/clickhouse/values.yaml
clickhouse:
  enabled: true
  ## @param clusterDomain Kubernetes cluster domain name
  ##
  clusterDomain: cluster.local
  ## @param extraDeploy Array of extra objects to deploy with the release
  ##
  diagnosticMode.enabled: true
  ## @section ClickHouse Parameters
  ##

  ## Bitnami ClickHouse image
  ## ref: https://hub.docker.com/r/bitnami/clickhouse/tags/
  ## @param image.registry ClickHouse image registry
  ## @param image.repository ClickHouse image repository
  ## @param image.tag ClickHouse image tag (immutable tags are recommended)
  ## @param image.digest ClickHouse image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param image.pullPolicy ClickHouse image pull policy
  ## @param image.pullSecrets ClickHouse image pull secrets
  ## @param image.debug Enable ClickHouse image debug mode
  ##
  image:
    registry: docker.io
    repository: bitnami/clickhouse
    tag: 23.5.3-debian-11-r0
    digest: ""
    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
    ## Enable debug mode
    ##
    debug: true
  ## @param shards Number of ClickHouse shards to deploy
  ##
  shards: 1

  ## @param replicaCount Number of ClickHouse replicas per shard to deploy
  ## if keeper enable, same as keeper count, keeper cluster by shards.
  ##
  replicaCount: 1
  ## @param containerPorts.http ClickHouse HTTP container port
  ## @param containerPorts.https ClickHouse HTTPS container port
  ## @param containerPorts.tcp ClickHouse TCP container port
  ## @param containerPorts.tcpSecure ClickHouse TCP (secure) container port
  ## @param containerPorts.keeper ClickHouse keeper TCP container port
  ## @param containerPorts.keeperSecure ClickHouse keeper TCP (secure) container port
  ## @param containerPorts.keeperInter ClickHouse keeper interserver TCP container port
  ## @param containerPorts.mysql ClickHouse MySQL container port
  ## @param containerPorts.postgresql ClickHouse PostgreSQL container port
  ## @param containerPorts.interserver ClickHouse Interserver container port
  ## @param containerPorts.metrics ClickHouse metrics container port
  ##
  containerPorts:
    http: 8123
    https: 8443
    tcp: 9000
    tcpSecure: 9440
    keeper: 2181
    keeperSecure: 3181
    keeperInter: 9444
    mysql: 9004
    postgresql: 9005
    interserver: 9009
    metrics: 8001
  ## Configure extra options for ClickHouse containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param livenessProbe.enabled Enable livenessProbe on ClickHouse containers
  ## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1
  ## @param readinessProbe.enabled Enable readinessProbe on ClickHouse containers
  ## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1
  ## @param startupProbe.enabled Enable startupProbe on ClickHouse containers
  ## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param startupProbe.periodSeconds Period seconds for startupProbe
  ## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1
  resources:
    limits: {}
    requests: {}
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param podSecurityContext.enabled Enabled ClickHouse pods' Security Context
  ## @param podSecurityContext.fsGroup Set ClickHouse pod's Security Context fsGroup
  ## @param podSecurityContext.seccompProfile.type Set ClickHouse container's Security Context seccomp profile
  ## If you are using Kubernetes 1.18, the following code needs to be commented out.
  ##
  podSecurityContext:
    enabled: true
    fsGroup: 1001
    seccompProfile:
      type: "RuntimeDefault"
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## @param containerSecurityContext.enabled Enabled ClickHouse containers' Security Context
  ## @param containerSecurityContext.runAsUser Set ClickHouse containers' Security Context runAsUser
  ## @param containerSecurityContext.runAsNonRoot Set ClickHouse containers' Security Context runAsNonRoot
  ## @param containerSecurityContext.allowPrivilegeEscalation Set ClickHouse container's privilege escalation
  ## @param containerSecurityContext.capabilities.drop Set ClickHouse container's Security Context runAsNonRoot
  ##
  containerSecurityContext:
    enabled: true
    runAsUser: 1001
    runAsNonRoot: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]

  ## Authentication
  ## @param auth.username ClickHouse Admin username
  ## @param auth.password ClickHouse Admin password
  ## @param auth.existingSecret Name of a secret containing the Admin password
  ## @param auth.existingSecretKey Name of the key inside the existing secret
  ##
  auth:
    username: default
    password: "supersecret"
    existingSecret: ""
    existingSecretKey: ""

  ## @param logLevel Logging level
  ##
  logLevel: information

  ## @section ClickHouse keeper configuration parameters
  ## @param keeper.enabled Deploy ClickHouse keeper. Support is experimental.
  ##
  keeper:
    enabled: false

  # existingOverridesConfigmap: "clickhouse-config"
  # extraOverridesConfigmap: ""

  ## @param defaultConfigurationOverrides [string] Default configuration overrides (evaluated as a template)
  ##
  defaultConfigurationOverrides: |
    <clickhouse>
      <!-- Macros -->
      <macros>
          <shard>01</shard>
          <replica>ch1</replica>
      </macros>
      <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>
      <max_session_timeout>3600</max_session_timeout>
      <default_session_timeout>60</default_session_timeout>
      <!-- Log Level -->
      <logger>
        <level>{{ .Values.logLevel }}</level>
      </logger>
      
      <!-- Cluster configuration - Any update of the shards and replicas requires helm upgrade -->
      <remote_servers>
        <featbit_ch_cluster>
          {{- $shards := $.Values.shards | int }}
          {{- range $shard, $e := until $shards }}
          <shard>
              {{- $replicas := $.Values.replicaCount | int }}
              {{- range $i, $_e := until $replicas }}
              <replica>
                  <host>{{ printf "%s-shard%d-%d.%s.%s.svc.%s" (include "common.names.fullname" $ ) $shard $i (include "clickhouse.headlessServiceName" $) (include "common.names.namespace" $) "cluster.local" }}</host>
                  <port>{{ $.Values.service.ports.tcp }}</port>
              </replica>
              {{- end }}
          </shard>
          {{- end }}
        </featbit_ch_cluster>
      </remote_servers>
      
      {{- if .Values.keeper.enabled }}
      <!-- keeper configuration -->
      <keeper_server>
        {{/*ClickHouse keeper configuration using the helm chart */}}
        <tcp_port>{{ $.Values.containerPorts.keeper }}</tcp_port>
        {{- if .Values.tls.enabled }}
        <tcp_port_secure>{{ $.Values.containerPorts.keeperSecure }}</tcp_port_secure>
        {{- end }}
        <server_id from_env="KEEPER_SERVER_ID"></server_id>
        <log_storage_path>/bitnami/clickhouse/keeper/coordination/log</log_storage_path>
        <snapshot_storage_path>/bitnami/clickhouse/keeper/coordination/snapshots</snapshot_storage_path>

        <coordination_settings>
            <operation_timeout_ms>10000</operation_timeout_ms>
            <session_timeout_ms>30000</session_timeout_ms>
            <raft_logs_level>trace</raft_logs_level>
        </coordination_settings>

        <raft_configuration>
        {{- $nodes := .Values.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <server>
          <id>{{ $node | int }}</id>
          <hostname from_env="{{ printf "KEEPER_NODE_%d" $node }}"></hostname>
          <port>{{ $.Values.service.ports.keeperInter }}</port>
        </server>
        {{- end }}
        </raft_configuration>
      </keeper_server>
      {{- end }}
      {{- if or .Values.keeper.enabled .Values.zookeeper.enabled .Values.externalZookeeper.servers }}
      <!-- Zookeeper configuration -->
      <zookeeper>
        {{- if or .Values.keeper.enabled }}
        {{- $nodes := .Values.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <node>
          <host from_env="{{ printf "KEEPER_NODE_%d" $node }}"></host>
          <port>{{ $.Values.service.ports.keeper }}</port>
        </node>
        {{- end }}
        {{- else if .Values.zookeeper.enabled }}
        {{/* Zookeeper configuration using the helm chart */}}
        {{- $nodes := .Values.zookeeper.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <node>
          <host from_env="{{ printf "KEEPER_NODE_%d" $node }}"></host>
          <port>{{ $.Values.zookeeper.service.ports.client }}</port>
        </node>
        {{- end }}
        {{- else if .Values.externalZookeeper.servers }}
        {{/* Zookeeper configuration using an external instance */}}
        {{- range $node :=.Values.externalZookeeper.servers }}
        <node>
          <host>{{ $node }}</host>
          <port>{{ $.Values.externalZookeeper.port }}</port>
        </node>
        {{- end }}
        {{- end }}
      </zookeeper>
      {{- end }}
      {{- if .Values.tls.enabled }}
      <!-- TLS configuration -->
      <tcp_port_secure from_env="CLICKHOUSE_TCP_SECURE_PORT"></tcp_port_secure>
      <https_port from_env="CLICKHOUSE_HTTPS_PORT"></https_port>
      <openSSL>
          <server>
              {{- $certFileName := default "tls.crt" .Values.tls.certFilename }}
              {{- $keyFileName := default "tls.key" .Values.tls.certKeyFilename }}
              <certificateFile>/bitnami/clickhouse/certs/{{$certFileName}}</certificateFile>
              <privateKeyFile>/bitnami/clickhouse/certs/{{$keyFileName}}</privateKeyFile>
              <verificationMode>none</verificationMode>
              <cacheSessions>true</cacheSessions>
              <disableProtocols>sslv2,sslv3</disableProtocols>
              <preferServerCiphers>true</preferServerCiphers>
              {{- if or .Values.tls.autoGenerated .Values.tls.certCAFilename }}
              {{- $caFileName := default "ca.crt" .Values.tls.certCAFilename }}
              <caConfig>/bitnami/clickhouse/certs/{{$caFileName}}</caConfig>
              {{- else }}
              <loadDefaultCAFile>true</loadDefaultCAFile>
              {{- end }}
          </server>
          <client>
              <loadDefaultCAFile>true</loadDefaultCAFile>
              <cacheSessions>true</cacheSessions>
              <disableProtocols>sslv2,sslv3</disableProtocols>
              <preferServerCiphers>true</preferServerCiphers>
              <verificationMode>none</verificationMode>
              <invalidCertificateHandler>
                  <name>AcceptCertificateHandler</name>
              </invalidCertificateHandler>
          </client>
      </openSSL>
      {{- end }}
      <max_concurrent_queries>1000</max_concurrent_queries>
      <max_server_memory_usage>0</max_server_memory_usage>
      <max_thread_pool_size>10000</max_thread_pool_size>
      <max_server_memory_usage_to_ram_ratio>0.9</max_server_memory_usage_to_ram_ratio>
      <total_memory_profiler_step>4194304</total_memory_profiler_step>
      <total_memory_tracker_sample_probability>0</total_memory_tracker_sample_probability>
      <uncompressed_cache_size>8589934592</uncompressed_cache_size>
      <mark_cache_size>5368709120</mark_cache_size>
      <mmap_cache_size>1000</mmap_cache_size>
      <compiled_expression_cache_size>134217728</compiled_expression_cache_size>
      <compiled_expression_cache_elements_size>10000</compiled_expression_cache_elements_size>
      <default_profile>default</default_profile>
      <default_database>default</default_database>
      <mlock_executable>true</mlock_executable>
      <remap_executable>false</remap_executable>
      {{- if .Values.metrics.enabled }}
      <!-- Prometheus metrics -->
      <prometheus>
          <endpoint>/metrics</endpoint>
          <port from_env="CLICKHOUSE_METRICS_PORT"></port>
          <metrics>true</metrics>
          <events>true</events>
          <asynchronous_metrics>true</asynchronous_metrics>
      </prometheus>
      {{- end }}
    </clickhouse>



  ## @param command Override default container command (useful when using custom images)
  ##
  command:
    - /scripts/setup.sh

  ## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft

  updateStrategy:
    ## StrategyType
    ## Can be set to RollingUpdate or OnDelete
    ##
    type: RollingUpdate

  ## @param podManagementPolicy Statefulset Pod management policy, it needs to be Parallel to be able to complete the cluster join
  ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  ##
  podManagementPolicy: Parallel

  ## @section Traffic Exposure Parameters
  ##

  ## ClickHouse service parameters
  ##
  service:
    ## @param service.type ClickHouse service type
    ##
    type: ClusterIP
    ## @param service.ports.http ClickHouse service HTTP port
    ## @param service.ports.https ClickHouse service HTTPS port
    ## @param service.ports.tcp ClickHouse service TCP port
    ## @param service.ports.tcpSecure ClickHouse service TCP (secure) port
    ## @param service.ports.keeper ClickHouse keeper TCP container port
    ## @param service.ports.keeperSecure ClickHouse keeper TCP (secure) container port
    ## @param service.ports.keeperInter ClickHouse keeper interserver TCP container port
    ## @param service.ports.mysql ClickHouse service MySQL port
    ## @param service.ports.postgresql ClickHouse service PostgreSQL port
    ## @param service.ports.interserver ClickHouse service Interserver port
    ## @param service.ports.metrics ClickHouse service metrics port
    ##
    ports:
      http: 8123
      https: 443
      tcp: 9000
      tcpSecure: 9440
      keeper: 2181
      keeperSecure: 3181
      keeperInter: 9444
      mysql: 9004
      postgresql: 9005
      interserver: 9009
      metrics: 8001

    ## @param service.externalTrafficPolicy ClickHouse service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/user-guide/services/
    ##
    sessionAffinity: None

  ## @section Persistence Parameters
  ##

  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    ## @param persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: true
    ## @param persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.labels Persistent Volume Claim labels
    ##
    labels: {}
    ## @param persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size Size of data volume
    ##
    size: 8Gi
    ## @param persistence.selector Selector to match an existing Persistent Volume for WordPress data PVC
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
    ## @param persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
  ## @section Init Container Parameters
  ##


  ## ServiceAccount configuration
  ##
  serviceAccount:
    ## @param serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: true
    ## @param serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param serviceAccount.automountServiceAccountToken Automount service account token for the server service account
    ##
    automountServiceAccountToken: true


  ## @section External Zookeeper paramaters
  ##
  externalZookeeper:
    ## @param externalZookeeper.servers List of external zookeeper servers to use
    ## @param externalZookeeper.port Port of the Zookeeper servers
    ##
    servers: [featbit-zookeeper]
    port: 2181

  ## @section Zookeeper subchart parameters
  ##
  ## @param zookeeper.enabled Deploy Zookeeper subchart
  ## @param zookeeper.replicaCount Number of Zookeeper instances
  ## @param zookeeper.service.ports.client Zookeeper client port
  ##
  zookeeper:
    enabled: false
    replicaCount: 1
    service:
      ports:
        client: 2181

## This value is only used when clickhouse.enabled is set to false
##
externalClickhouse:
  ## Hostname or ip address of external clickhouse
  ##
  host: "clickhouse"
  tcpPort: 9000
  httpPort: 8123
  username: default
  password: ""
  database: default
  singleNode: true
